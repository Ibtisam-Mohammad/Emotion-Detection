{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"0_10\": {\"positive\": 1, \"neutral\": 2, \"negative\": 3}, \"10_20\": {\"positive\": 1, \"neutral\": 2, \"negative\": 3}, \"20_30\": {\"positive\": 1, \"neutral\": 2, \"negative\": 3}, \"30_40\": {\"positive\": 1, \"neutral\": 2, \"negative\": 3}, \"40_50\": {\"positive\": 1, \"neutral\": 2, \"negative\": 3}, \"50_60\": {\"positive\": 1, \"neutral\": 2, \"negative\": 3}, \"70_80\": {\"positive\": 1, \"neutral\": 2, \"negative\": 3}, \"80_90\": {\"positive\": 1, \"neutral\": 2, \"negative\": 3}}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "dir_ ={\n",
    "    \"0_10\":{\"positive\":1,\"neutral\":2,\"negative\":3},\n",
    "    \"10_20\":{\"positive\":1,\"neutral\":2,\"negative\":3},\"10_20\":{\"positive\":1,\"neutral\":2,\"negative\":3},\n",
    "\"20_30\":{\"positive\":1,\"neutral\":2,\"negative\":3},\"30_40\":{\"positive\":1,\"neutral\":2,\"negative\":3},\"40_50\":{\"positive\":1,\"neutral\":2,\"negative\":3},\n",
    "\"50_60\":{\"positive\":1,\"neutral\":2,\"negative\":3},\"70_80\":{\"positive\":1,\"neutral\":2,\"negative\":3},\"80_90\":{\"positive\":1,\"neutral\":2,\"negative\":3}\n",
    "  }\n",
    "json.dumps(dir_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0_10': {'positive': 1, 'neutral': 2, 'negative': 3},\n",
       " '10_20': {'positive': 1, 'neutral': 2, 'negative': 3},\n",
       " '20_30': {'positive': 1, 'neutral': 2, 'negative': 3},\n",
       " '30_40': {'positive': 1, 'neutral': 2, 'negative': 3},\n",
       " '40_50': {'positive': 1, 'neutral': 2, 'negative': 3},\n",
       " '50_60': {'positive': 1, 'neutral': 2, 'negative': 3},\n",
       " '70_80': {'positive': 1, 'neutral': 2, 'negative': 3},\n",
       " '80_90': {'positive': 1, 'neutral': 2, 'negative': 3}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Programs\\Anaconda_files\\envs\\yolo\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Yolov7\\YOLO\\Y\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import subprocess\n",
    "# from bucket import download_blob, list_blobs\n",
    "import pickle\n",
    "from detect_trial import detect\n",
    "import os \n",
    "print(os.getcwd())\n",
    "# download_blob('edaa_bucket', 'directory.pickle', 'directory.pickle')\n",
    "# with open('directory.pickle','rb') as f:\n",
    "#     dir_=pickle.load(f)\n",
    "# for file_name in dir_.keys():\n",
    "#     download_blob('edaa_bucket','chunks/'+file_name, 'input_dir/'+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv7  2022-10-7 torch 1.11.0 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 314 layers, 36481772 parameters, 6194944 gradients\n",
      "e:\\Programs\\Anaconda_files\\envs\\yolo\\lib\\site-packages\\torch\\functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 faces, Done. (8219.1ms) Inference, (295.8ms) NMS\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m detect(weights\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mweights/face.pt\u001b[39;49m\u001b[39m'\u001b[39;49m,source\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39minference/images\u001b[39;49m\u001b[39m'\u001b[39;49m,img_size\u001b[39m=\u001b[39;49m\u001b[39m640\u001b[39;49m,conf_thres\u001b[39m=\u001b[39;49m\u001b[39m0.50\u001b[39;49m,device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m,view_img\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\\\n\u001b[0;32m      2\u001b[0m     save_txt\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,save_conf\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,save_crop\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\\\n\u001b[0;32m      3\u001b[0m         project\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mruns/detect\u001b[39;49m\u001b[39m'\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexp\u001b[39;49m\u001b[39m'\u001b[39;49m,\\\n\u001b[0;32m      4\u001b[0m         no_trace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, classes\u001b[39m=\u001b[39;49m[\u001b[39m0\u001b[39;49m],save_img\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,nosave\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32md:\\Yolov7\\YOLO\\Y\\detect_trial.py:91\u001b[0m, in \u001b[0;36mdetect\u001b[1;34m(weights, source, img_size, conf_thres, iou_thres, device, view_img, save_txt, save_conf, save_crop, nosave, agnostic_nms, augment, update, project, name, exist_ok, no_trace, save_img, classes)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[0;32m     90\u001b[0m t1 \u001b[39m=\u001b[39m time_synchronized()\n\u001b[1;32m---> 91\u001b[0m pred \u001b[39m=\u001b[39m model(img, augment\u001b[39m=\u001b[39;49maugment)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     92\u001b[0m t2 \u001b[39m=\u001b[39m time_synchronized()\n\u001b[0;32m     94\u001b[0m \u001b[39m# Apply NMS\u001b[39;00m\n",
      "File \u001b[1;32me:\\Programs\\Anaconda_files\\envs\\yolo\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Yolov7\\YOLO\\Y\\models\\yolo.py:589\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x, augment, profile)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[39mfor\u001b[39;00m si, fi \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(s, f):\n\u001b[0;32m    588\u001b[0m     xi \u001b[39m=\u001b[39m scale_img(x\u001b[39m.\u001b[39mflip(fi) \u001b[39mif\u001b[39;00m fi \u001b[39melse\u001b[39;00m x, si, gs\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride\u001b[39m.\u001b[39mmax()))\n\u001b[1;32m--> 589\u001b[0m     yi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_once(xi)[\u001b[39m0\u001b[39m]  \u001b[39m# forward\u001b[39;00m\n\u001b[0;32m    590\u001b[0m     \u001b[39m# cv2.imwrite(f'img_{si}.jpg', 255 * xi[0].cpu().numpy().transpose((1, 2, 0))[:, :, ::-1])  # save\u001b[39;00m\n\u001b[0;32m    591\u001b[0m     yi[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :\u001b[39m4\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m si  \u001b[39m# de-scale\u001b[39;00m\n",
      "File \u001b[1;32md:\\Yolov7\\YOLO\\Y\\models\\yolo.py:625\u001b[0m, in \u001b[0;36mModel.forward_once\u001b[1;34m(self, x, profile)\u001b[0m\n\u001b[0;32m    622\u001b[0m         dt\u001b[39m.\u001b[39mappend((time_synchronized() \u001b[39m-\u001b[39m t) \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m)\n\u001b[0;32m    623\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m%10.1f\u001b[39;00m\u001b[39m%10.0f\u001b[39;00m\u001b[39m%10.1f\u001b[39;00m\u001b[39mms \u001b[39m\u001b[39m%-40s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (o, m\u001b[39m.\u001b[39mnp, dt[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], m\u001b[39m.\u001b[39mtype))\n\u001b[1;32m--> 625\u001b[0m     x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[39mif\u001b[39;00m profile:\n",
      "File \u001b[1;32me:\\Programs\\Anaconda_files\\envs\\yolo\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Yolov7\\YOLO\\Y\\models\\common.py:111\u001b[0m, in \u001b[0;36mConv.fuseforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfuseforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 111\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x))\n",
      "File \u001b[1;32me:\\Programs\\Anaconda_files\\envs\\yolo\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\Programs\\Anaconda_files\\envs\\yolo\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 447\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32me:\\Programs\\Anaconda_files\\envs\\yolo\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    440\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    441\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    442\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 443\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    444\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "detect(weights='weights/face.pt',source='inference/images',img_size=640,conf_thres=0.50,device='cpu',view_img=False,\\\n",
    "    save_txt=False,save_conf=False,save_crop=True,\\\n",
    "        project='runs/detect', name='exp',\\\n",
    "        no_trace=True, classes=[0],save_img=False,nosave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv7  2022-10-7 torch 1.11.0 CUDA:0 (NVIDIA GeForce RTX 3060, 12287.375MB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "IAuxDetect.fuse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 819 layers, 164950868 parameters, 0 gradients, 225.6 GFLOPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 neutral, Done. (229.0ms) Inference, (3.0ms) NMS\n",
      " The image with the result is saved in: runs\\detect\\exp1\\bus.jpg\n",
      "1 neutral, Done. (173.0ms) Inference, (2.0ms) NMS\n",
      " The image with the result is saved in: runs\\detect\\exp1\\horses.jpg\n",
      "1 surprise, Done. (186.0ms) Inference, (4.0ms) NMS\n",
      " The image with the result is saved in: runs\\detect\\exp1\\image1.jpg\n",
      "1 neutral, 1 surprise, Done. (187.0ms) Inference, (1.0ms) NMS\n",
      " The image with the result is saved in: runs\\detect\\exp1\\image2.jpg\n",
      "1 neutral, Done. (270.0ms) Inference, (1.0ms) NMS\n",
      " The image with the result is saved in: runs\\detect\\exp1\\image3.jpg\n",
      "Done. (158.0ms) Inference, (1.0ms) NMS\n",
      " The image with the result is saved in: runs\\detect\\exp1\\zidane.jpg\n",
      "Done. (5.414s)\n"
     ]
    }
   ],
   "source": [
    "detect(weights='emotions.pt',source='inference/images',img_size=640,conf_thres=0.50,device='0',view_img=False,\\\n",
    "    save_txt=False,save_conf=False,save_crop=True,\\\n",
    "        project='runs/detect', name='exp1',\\\n",
    "        no_trace=True, classes=[0,1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('yolo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3095774132eab8dbcea1ada5bd91a2ef851802cdc01b2ff8fbd751baceb5f08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
